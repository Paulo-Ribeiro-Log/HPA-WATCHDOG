# DetecÃ§Ã£o de Anomalias - HPA Watchdog

EstratÃ©gia completa de detecÃ§Ã£o de comportamentos anormais em HPAs e Deployments.

## ğŸ“‹ Ãndice

- [Filosofia](#filosofia)
- [Categorias de Anomalias](#categorias-de-anomalias)
  - [1. Anomalias de Escalonamento](#1-anomalias-de-escalonamento-hpa-behavior)
  - [2. Anomalias de Deployment/Pods](#2-anomalias-de-deploymentpods-application-health)
  - [3. Anomalias de MÃ©tricas](#3-anomalias-de-mÃ©tricas-performance)
  - [4. Anomalias de ConfiguraÃ§Ã£o](#4-anomalias-de-configuraÃ§Ã£o-config-issues)
- [Limites e Thresholds](#limites-e-thresholds-recomendados)
- [Roadmap de ImplementaÃ§Ã£o](#roadmap-de-implementaÃ§Ã£o)
- [Ideias Futuras](#ideias-futuras)

---

## Filosofia

### **Signal vs Noise**

O objetivo Ã© **alertar apenas o que importa** - problemas reais que precisam de aÃ§Ã£o.

**PrincÃ­pios:**
- âœ… **Actionable alerts** - Cada alerta deve sugerir uma aÃ§Ã£o clara
- âŒ **Evitar alert fatigue** - Muitos alertas = todos ignorados
- ğŸ¯ **Root cause focus** - Identificar a causa raiz, nÃ£o sintomas
- â±ï¸ **Time-based validation** - Confirmar problema antes de alertar
- ğŸ”• **Cooldown periods** - NÃ£o spam de alertas repetidos

### **NÃ­veis de Severidade**

```
ğŸ”´ CRITICAL - Requer aÃ§Ã£o imediata
   Exemplos: OOMKilled, CrashLoop, Maxed Out

ğŸŸ¡ WARNING - Requer atenÃ§Ã£o mas nÃ£o urgente
   Exemplos: Underutilization, Inefficient Config

ğŸ”µ INFO - Informativo, sem aÃ§Ã£o necessÃ¡ria
   Exemplos: Config changes, Scale events
```

---

## Categorias de Anomalias

## 1ï¸âƒ£ Anomalias de Escalonamento (HPA Behavior)

### ğŸ”´ CRÃTICAS

#### A) Thrashing / Oscillation

**DescriÃ§Ã£o:** HPA escalando up/down rapidamente (comportamento instÃ¡vel)

**DetecÃ§Ã£o:**
```yaml
condition:
  replica_changes: > 5
  time_window: 5 minutos

exemplo:
  t=0:00  â†’ 3 replicas
  t=0:30  â†’ 5 replicas â†‘â†‘
  t=1:00  â†’ 3 replicas â†“â†“
  t=2:00  â†’ 6 replicas â†‘â†‘â†‘
  t=3:00  â†’ 4 replicas â†“â†“
  t=4:00  â†’ 7 replicas â†‘â†‘â†‘

resultado: ğŸš¨ 6 mudanÃ§as em 5 minutos = THRASHING
```

**Causas Comuns:**
- HPA targets muito sensÃ­veis (ex: 50% CPU)
- MÃ©tricas com spikes artificiais (ex: batch jobs)
- `stabilizationWindow` muito curto
- MÃ©tricas instÃ¡veis (ex: network I/O de API externa)

**Impacto:**
- âš ï¸ Pods sendo criados/destruÃ­dos constantemente
- ğŸ’¸ DesperdÃ­cio de recursos
- ğŸ› Bugs por inicializaÃ§Ã£o/finalizaÃ§Ã£o frequente
- ğŸ“Š MÃ©tricas inconsistentes

**AÃ§Ã£o Sugerida:**
```yaml
1. Revisar targets do HPA:
   - CPU target < 70%? Considere aumentar para 70-80%
   - Memory target < 80%? Considere aumentar

2. Aumentar stabilizationWindow:
   behavior:
     scaleDown:
       stabilizationWindowSeconds: 300  # 5 minutos

3. Verificar spikes artificiais:
   - Filtrar mÃ©tricas de batch jobs
   - Usar average em vez de max

4. Considerar custom metrics mais estÃ¡veis:
   - Request rate em vez de CPU
   - Queue depth em vez de memory
```

**Threshold ConfigurÃ¡vel:**
```yaml
anomaly_detection:
  scaling:
    oscillation:
      max_changes: 5           # MÃ¡ximo de mudanÃ§as permitidas
      window_minutes: 5        # Janela de tempo
      severity: critical       # NÃ­vel de severidade
      cooldown_minutes: 10     # NÃ£o alertar novamente por 10min
```

---

#### B) Maxed Out

**DescriÃ§Ã£o:** HPA atingiu `maxReplicas` mas mÃ©tricas continuam altas

**DetecÃ§Ã£o:**
```yaml
condition:
  current_replicas: == maxReplicas
  AND:
    - cpu_current: > cpu_target + 20%
    OR:
    - memory_current: > memory_target + 20%
  duration: > 2 minutos consecutivos

exemplo:
  maxReplicas: 10
  currentReplicas: 10        âœ… Maxed out
  cpuTarget: 70%
  cpuCurrent: 92%            ğŸš¨ 22% acima do target!

resultado: ğŸš¨ HPA nÃ£o pode escalar mais mas carga alta
```

**Causas Comuns:**
- `maxReplicas` muito conservador
- Spike de trÃ¡fego alÃ©m da capacidade planejada
- Problema de performance (nÃ£o resolve com escala)

**Impacto:**
- ğŸ”¥ AplicaÃ§Ã£o sobrecarregada
- ğŸ˜¡ UsuÃ¡rios com latÃªncia alta ou erros
- ğŸ’¥ Risco de cascata de falhas

**AÃ§Ã£o Sugerida:**
```yaml
IMEDIATA:
  1. Aumentar maxReplicas temporariamente:
     kubectl patch hpa <name> -p '{"spec":{"maxReplicas":15}}'

  2. Verificar se hÃ¡ problemas de performance:
     - Queries SQL lentas?
     - Memory leaks?
     - CÃ³digo ineficiente?

LONGO PRAZO:
  1. Ajustar maxReplicas baseado em carga esperada:
     maxReplicas = peak_load / capacity_per_pod * 1.5

  2. Considerar horizontal + vertical scaling

  3. Otimizar aplicaÃ§Ã£o se problema nÃ£o Ã© de escala
```

**Threshold ConfigurÃ¡vel:**
```yaml
anomaly_detection:
  scaling:
    maxed_out:
      cpu_deviation_percent: 20      # Quanto acima do target
      memory_deviation_percent: 20   # Quanto acima do target
      duration_minutes: 2            # Por quanto tempo
      severity: critical
```

---

#### C) Scaling Stuck

**DescriÃ§Ã£o:** MÃ©tricas altas mas HPA nÃ£o consegue escalar

**DetecÃ§Ã£o:**
```yaml
condition:
  cpu_current: > cpu_target + 30%
  OR:
  memory_current: > memory_target + 30%

  AND:
  desired_replicas: == current_replicas  # NÃ£o mudou
  duration: > 5 minutos

exemplo:
  cpuTarget: 70%
  cpuCurrent: 105%           ğŸš¨ 35% acima!
  desiredReplicas: 5
  currentReplicas: 5         âš ï¸ NÃ£o escalou

resultado: ğŸš¨ HPA quer escalar mas nÃ£o consegue
```

**Causas Comuns:**
- **ResourceQuota excedida** no namespace
- **Insufficient resources** no cluster (nodes cheios)
- **Pod disruption budget** impedindo
- **Taints/tolerations** impedindo scheduling
- HPA `conditions` com erro

**Impacto:**
- ğŸ”¥ AplicaÃ§Ã£o sobrecarregada sem conseguir escalar
- ğŸ“‰ SLA impactado
- ğŸ˜¡ UsuÃ¡rios insatisfeitos

**AÃ§Ã£o Sugerida:**
```yaml
DIAGNÃ“STICO:
  1. Verificar events do HPA:
     kubectl describe hpa <name>

  2. Verificar conditions:
     kubectl get hpa <name> -o yaml | grep conditions -A 10

  3. Verificar quotas do namespace:
     kubectl describe quota -n <namespace>

  4. Verificar capacidade do cluster:
     kubectl top nodes
     kubectl describe nodes | grep -A 5 "Allocated resources"

AÃ‡ÃƒO:
  - Quota excedida â†’ Aumentar quota ou limpar recursos
  - Nodes cheios â†’ Adicionar nodes ao cluster
  - PDB restritivo â†’ Ajustar PodDisruptionBudget
  - Taints â†’ Adicionar tolerations aos pods
```

**Threshold ConfigurÃ¡vel:**
```yaml
anomaly_detection:
  scaling:
    stuck:
      cpu_deviation_percent: 30
      memory_deviation_percent: 30
      duration_minutes: 5
      severity: critical
```

---

### ğŸŸ¡ WARNINGS

#### D) Underutilization

**DescriÃ§Ã£o:** RÃ©plicas altas mas mÃ©tricas muito baixas (desperdÃ­cio)

**DetecÃ§Ã£o:**
```yaml
condition:
  cpu_current: < cpu_target - 40%
  OR:
  memory_current: < memory_target - 40%

  AND:
  current_replicas: > minReplicas + 3
  duration: > 15 minutos

exemplo:
  cpuTarget: 70%
  cpuCurrent: 25%            âš ï¸ 45% abaixo!
  currentReplicas: 8
  minReplicas: 2

resultado: ğŸŸ¡ DesperdÃ­cio de recursos
```

**Causas Comuns:**
- Spike de carga passou mas HPA ainda nÃ£o scaled down
- `scaleDown.stabilizationWindow` muito longo
- TrÃ¡fego sazonal (horÃ¡rio de baixa)
- `maxReplicas` ou `target` mal configurados

**Impacto:**
- ğŸ’¸ Custo desnecessÃ¡rio (pods ociosos)
- ğŸ“Š MÃ©tricas enganosas (baixa utilizaÃ§Ã£o)

**AÃ§Ã£o Sugerida:**
```yaml
SE TEMPORÃRIO (horÃ¡rio baixa):
  - Aguardar HPA scale down naturalmente
  - Considerar scheduled scaling (ex: CronJob)

SE PERSISTENTE:
  1. Reduzir maxReplicas:
     spec:
       maxReplicas: 5  # Era 10

  2. Ou ajustar target:
     spec:
       metrics:
       - type: Resource
         resource:
           name: cpu
           target:
             type: Utilization
             averageUtilization: 80  # Era 70

  3. Ou reduzir stabilizationWindow:
     behavior:
       scaleDown:
         stabilizationWindowSeconds: 60  # Era 300
```

**Threshold ConfigurÃ¡vel:**
```yaml
anomaly_detection:
  scaling:
    underutilization:
      cpu_deviation_percent: 40
      memory_deviation_percent: 40
      duration_minutes: 15
      min_excess_replicas: 3   # SÃ³ alerta se >3 rÃ©plicas alÃ©m do min
      severity: warning
```

---

#### E) Frequent Scaling

**DescriÃ§Ã£o:** Escala com frequÃªncia mas nÃ£o Ã© thrashing

**DetecÃ§Ã£o:**
```yaml
condition:
  replica_changes: 3-4
  time_window: 10 minutos

exemplo:
  t=0   â†’ 3 replicas
  t=3m  â†’ 4 replicas â†‘
  t=6m  â†’ 5 replicas â†‘
  t=9m  â†’ 4 replicas â†“

resultado: ğŸŸ¡ 4 mudanÃ§as em 10min (instabilidade leve)
```

**Impacto:**
- ğŸ”„ Churn moderado de pods
- ğŸ“Š MÃ©tricas um pouco instÃ¡veis

**AÃ§Ã£o Sugerida:**
```yaml
1. Aumentar stabilizationWindow:
   behavior:
     scaleDown:
       stabilizationWindowSeconds: 180
```

---

## 2ï¸âƒ£ Anomalias de Deployment/Pods (Application Health)

### ğŸ”´ CRÃTICAS

#### A) Pod CrashLoopBackOff

**DescriÃ§Ã£o:** Pods reiniciando continuamente apÃ³s falhas

**DetecÃ§Ã£o:**
```yaml
condition:
  ANY pod:
    status: CrashLoopBackOff
  OR:
    restart_count: > 5
    time_window: 10 minutos

exemplo:
  pod-1: Running, restarts=0
  pod-2: CrashLoopBackOff, restarts=8  ğŸš¨
  pod-3: Running, restarts=1

resultado: ğŸš¨ Pod em crash loop
```

**Causas Comuns:**
- **Application error** - CÃ³digo crashando
- **Missing dependencies** - DB inacessÃ­vel, secret faltando
- **Liveness probe failure** - App nÃ£o responde a tempo
- **Resource limits** - CPU/Memory insuficiente
- **Config error** - VariÃ¡vel de ambiente errada

**Impacto:**
- ğŸ“‰ Capacidade reduzida (pods nÃ£o funcionais)
- ğŸ”¥ Pode afetar todo deployment se muitos pods crasham
- ğŸ˜¡ UsuÃ¡rios impactados

**CorrelaÃ§Ã£o Comum:**
```
ğŸš¨ HPA escalou para 10 replicas
âš ï¸  Mas 6 pods em CrashLoopBackOff
ğŸ’¡ Problema NÃƒO Ã© carga, Ã© cÃ³digo/configuraÃ§Ã£o!
```

**AÃ§Ã£o Sugerida:**
```yaml
DIAGNÃ“STICO:
  1. Ver logs do pod:
     kubectl logs <pod-name> --previous

  2. Ver events:
     kubectl describe pod <pod-name>

  3. Verificar liveness probe:
     kubectl get pod <pod-name> -o yaml | grep liveness -A 5

CAUSAS E SOLUÃ‡Ã•ES:
  Application error:
    â†’ Verificar logs e corrigir cÃ³digo

  Missing dependencies:
    â†’ Verificar conectividade: kubectl exec <pod> -- curl <db-url>
    â†’ Verificar secrets: kubectl get secret <name>

  Liveness probe falha:
    â†’ Aumentar initialDelaySeconds
    â†’ Aumentar timeoutSeconds

  Resource limits:
    â†’ Aumentar CPU/Memory limits
```

**Threshold ConfigurÃ¡vel:**
```yaml
anomaly_detection:
  pods:
    crash_loop:
      max_restarts: 5
      window_minutes: 10
      severity: critical
      immediate: true  # Alerta imediato se status=CrashLoopBackOff
```

---

#### B) Pods OOMKilled

**DescriÃ§Ã£o:** Pods sendo mortos por falta de memÃ³ria

**DetecÃ§Ã£o:**
```yaml
condition:
  ANY pod:
    termination_reason: OOMKilled
  OR:
    memory_usage: > 95% of memory_limit

exemplo:
  pod-1: memory 450Mi / 512Mi (88%) âœ…
  pod-2: OOMKilled (was 510Mi / 512Mi) ğŸš¨

resultado: ğŸš¨ Pod killed por OOM
```

**Causas Comuns:**
- **Memory leak** - AplicaÃ§Ã£o nÃ£o libera memÃ³ria
- **Memory limit muito baixo** - App precisa de mais
- **Spike de uso** - Carga pontual alta
- **Large objects** - Cache, buffers grandes

**Impacto:**
- ğŸ’¥ Pod morto abruptamente (pode corromper dados)
- ğŸ“‰ Capacidade reduzida
- ğŸ”„ ReinÃ­cio constante se leak persistente

**CorrelaÃ§Ã£o Comum:**
```
ğŸš¨ HPA escalou para 10 replicas
âš ï¸  Mas todos pods OOMKilled apÃ³s alguns minutos
ğŸ’¡ Memory leak ou limit insuficiente!
```

**AÃ§Ã£o Sugerida:**
```yaml
INVESTIGAR:
  1. Verificar histÃ³rico de memory usage:
     # Via Prometheus
     container_memory_working_set_bytes{pod="<pod>"}

  2. Verificar se Ã© leak:
     - Memory sobe continuamente?
     - Ou sobe atÃ© limit e estabiliza?

SE LIMIT INSUFICIENTE:
  resources:
    limits:
      memory: 1Gi  # Era 512Mi

SE MEMORY LEAK:
  â†’ Investigar cÃ³digo
  â†’ Usar profiler (pprof, heapdump)
  â†’ Corrigir leak

WORKAROUND TEMPORÃRIO:
  â†’ Aumentar memory limit
  â†’ Adicionar restart automÃ¡tico periÃ³dico (nÃ£o ideal!)
```

**Threshold ConfigurÃ¡vel:**
```yaml
anomaly_detection:
  pods:
    oom_killed:
      immediate: true          # Alerta imediato
      memory_threshold_percent: 95  # Alerta preventivo
      severity: critical
```

---

#### C) Pods Not Ready

**DescriÃ§Ã£o:** Pods existem mas nÃ£o estÃ£o prontos para receber trÃ¡fego

**DetecÃ§Ã£o:**
```yaml
condition:
  (available_replicas / current_replicas) < 70%
  duration: > 3 minutos

exemplo:
  currentReplicas: 10
  availableReplicas: 4       âš ï¸ Apenas 40% pronto!
  readyReplicas: 4

resultado: ğŸš¨ 60% dos pods nÃ£o estÃ£o ready
```

**Causas Comuns:**
- **Readiness probe failing** - App nÃ£o responde
- **Slow startup** - App demora muito pra iniciar
- **Dependencies unavailable** - DB, cache down
- **Resource constraints** - CPU throttling durante startup

**Impacto:**
- ğŸ“‰ Capacidade real muito menor que esperado
- âš ï¸ HPA pode escalar mais mas pods nÃ£o ficam ready
- ğŸ”¥ Sobrecarga nos pods que estÃ£o ready

**CorrelaÃ§Ã£o Comum:**
```
ğŸš¨ HPA escalou para 10 replicas
âš ï¸  Mas apenas 3 pods ready
ğŸ’¡ Readiness probe ou dependÃªncias falhando!
```

**AÃ§Ã£o Sugerida:**
```yaml
DIAGNÃ“STICO:
  1. Ver conditions dos pods:
     kubectl get pods -o wide
     kubectl describe pod <pod-name>

  2. Ver readiness probe:
     kubectl get pod <pod-name> -o yaml | grep readiness -A 5

  3. Testar probe manualmente:
     kubectl exec <pod> -- curl localhost:8080/health

SOLUÃ‡Ã•ES:
  Readiness probe timeout:
    readinessProbe:
      initialDelaySeconds: 30  # Era 10
      timeoutSeconds: 5        # Era 1
      periodSeconds: 10

  Startup lento:
    startupProbe:  # K8s 1.16+
      initialDelaySeconds: 0
      periodSeconds: 10
      failureThreshold: 30  # 5 minutos total

  DependÃªncias:
    â†’ Verificar conectividade
    â†’ Adicionar init containers se necessÃ¡rio
```

**Threshold ConfigurÃ¡vel:**
```yaml
anomaly_detection:
  pods:
    not_ready:
      ready_threshold_percent: 70
      duration_minutes: 3
      severity: critical
```

---

### ğŸŸ¡ WARNINGS

#### D) High Pod Restart Rate

**DescriÃ§Ã£o:** Pods reiniciando mas nÃ£o em crash loop total

**DetecÃ§Ã£o:**
```yaml
condition:
  restart_count: 2-4
  time_window: 15 minutos
  status: NOT CrashLoopBackOff

exemplo:
  pod-1: restarts=2 em 15min
  pod-2: restarts=3 em 15min

resultado: ğŸŸ¡ Taxa alta de restarts
```

**Causas Comuns:**
- OOM ocasional
- Liveness probe falhas ocasionais
- Deployment rollouts

**AÃ§Ã£o Sugerida:**
```yaml
1. Investigar logs para causa
2. Ajustar probes se necessÃ¡rio
3. Monitorar se evolui para crash loop
```

---

#### E) Slow Rollout

**DescriÃ§Ã£o:** Rolling update demorando muito

**DetecÃ§Ã£o:**
```yaml
condition:
  updated_replicas: < desired_replicas
  duration: > 10 minutos

exemplo:
  desiredReplicas: 10
  updatedReplicas: 4
  elapsed: 12 minutos

resultado: ğŸŸ¡ Rollout lento (40% apÃ³s 12min)
```

**Causas Comuns:**
- `maxUnavailable` muito conservador
- `maxSurge` = 0 (atualiza um por vez)
- Readiness probe com `initialDelaySeconds` alto
- Resources insuficientes para criar novos pods

**AÃ§Ã£o Sugerida:**
```yaml
Acelerar rollout:
  strategy:
    rollingUpdate:
      maxSurge: 25%        # Era 0
      maxUnavailable: 25%  # Era 1
```

---

## 3ï¸âƒ£ Anomalias de MÃ©tricas (Performance)

### ğŸ”´ CRÃTICAS

#### A) CPU Throttling Excessivo

**DescriÃ§Ã£o:** Pods sendo throttled (limitados) pela CPU

**DetecÃ§Ã£o:**
```yaml
condition:
  cpu_throttling: > 25%
  duration: > 5 minutos

cÃ¡lculo:
  throttling_percent = (throttled_time / cpu_time) * 100

exemplo:
  cpu_usage: 450m / 500m (90%)    âœ… Uso ok
  cpu_throttling: 35%              ğŸš¨ Muito throttled!

resultado: ğŸš¨ Performance degradada por throttling
```

**Causas:**
- CPU `limit` muito baixo
- Bursts de CPU batendo no limit
- CPU limit desnecessÃ¡rio

**Impacto:**
- ğŸ“‰ Performance ruim mesmo com CPU "disponÃ­vel"
- â±ï¸ LatÃªncia alta
- ğŸ› Timeouts

**CorrelaÃ§Ã£o Comum:**
```
ğŸ’¡ CPU estÃ¡ em 60% mas latÃªncia alta
ğŸš¨ CPU throttling em 40%!
â†’ Problema: limit muito baixo, nÃ£o falta de CPU
```

**AÃ§Ã£o Sugerida:**
```yaml
SOLUÃ‡ÃƒO RECOMENDADA:
  resources:
    requests:
      cpu: 500m
    # limits:
    #   cpu: 1000m  â† REMOVER limit de CPU!

  # CPU limits causam throttling desnecessÃ¡rio
  # Melhor: usar apenas requests + QoS Guaranteed

ALTERNATIVA (se limit necessÃ¡rio):
  resources:
    limits:
      cpu: 2000m  # Aumentar significativamente
```

**Threshold ConfigurÃ¡vel:**
```yaml
anomaly_detection:
  performance:
    cpu_throttling:
      threshold_percent: 25
      duration_minutes: 5
      severity: critical
```

---

#### B) High Error Rate

**DescriÃ§Ã£o:** Muitos erros HTTP (5xx)

**DetecÃ§Ã£o:**
```yaml
condition:
  error_rate: > 5%
  duration: > 2 minutos

cÃ¡lculo:
  error_rate = (5xx_count / total_requests) * 100

exemplo:
  total_requests: 1000 req/s
  5xx_errors: 75 req/s
  error_rate: 7.5%           ğŸš¨

resultado: ğŸš¨ Taxa de erro acima do aceitÃ¡vel
```

**Causas Comuns:**
- **Sobrecarga** - Mais carga que capacidade
- **DependÃªncias down** - DB, cache, API externa
- **Bug** - CÃ³digo com erro
- **Resources** - OOM, throttling

**Impacto:**
- ğŸ˜¡ UsuÃ¡rios recebendo erros
- ğŸ“‰ SLA impactado
- ğŸ’¸ Receita perdida

**CorrelaÃ§Ã£o Comum:**
```
ğŸš¨ Error rate 8% (>5% threshold)
ğŸ“Š HPA maxed out (10/10 replicas)
ğŸ’¡ Precisa escalar mais OU problema nÃ£o Ã© escala
```

**AÃ§Ã£o Sugerida:**
```yaml
SE HPA MAXED OUT:
  â†’ Aumentar maxReplicas
  â†’ Scale vertical (mais CPU/Memory por pod)

SE NÃƒO MAXED OUT:
  1. Verificar dependÃªncias:
     - DB latency alta?
     - Cache down?
     - API externa com erro?

  2. Verificar logs para causa:
     kubectl logs <pod> | grep "500\|error"

  3. Verificar se Ã© bug recente:
     - Rollout novo?
     - Config change?
```

**Threshold ConfigurÃ¡vel:**
```yaml
anomaly_detection:
  performance:
    error_rate:
      threshold_percent: 5
      duration_minutes: 2
      severity: critical
```

---

#### C) High Latency (P95)

**DescriÃ§Ã£o:** LatÃªncia P95 muito alta

**DetecÃ§Ã£o:**
```yaml
condition:
  p95_latency: > threshold_ms
  OR:
  p95_latency: > baseline * 2
  duration: > 3 minutos

exemplo:
  threshold: 1000ms
  baseline: 200ms
  current_p95: 1200ms        ğŸš¨ Acima threshold
  current_p95: 450ms         ğŸš¨ 2.25x baseline

resultado: ğŸš¨ LatÃªncia degradada
```

**Causas Comuns:**
- **Sobrecarga** - CPU/Memory high
- **Slow queries** - DB queries ineficientes
- **External API** - DependÃªncia lenta
- **GC pauses** - Garbage collection (Java, Go)
- **Network issues** - LatÃªncia de rede

**Impacto:**
- ğŸ˜¡ ExperiÃªncia do usuÃ¡rio ruim
- â±ï¸ Timeouts em cascata
- ğŸ“‰ Taxa de conversÃ£o menor

**CorrelaÃ§Ã£o Comum:**
```
ğŸš¨ P95 latency 1200ms (threshold 1000ms)
ğŸ“Š CPU 90% (target 70%)
ğŸ’¡ Escalar pode resolver
```

**AÃ§Ã£o Sugerida:**
```yaml
SE CPU/MEMORY HIGH:
  â†’ HPA deve escalar automaticamente
  â†’ Se maxed out, aumentar maxReplicas

SE CPU/MEMORY OK:
  1. Verificar queries lentas:
     # Prometheus query
     histogram_quantile(0.95,
       rate(http_request_duration_seconds_bucket[5m]))

  2. Verificar dependÃªncias externas:
     - DB latency?
     - Cache miss rate?
     - External API timeout?

  3. Profile da aplicaÃ§Ã£o:
     - Go: pprof
     - Java: JProfiler
     - Python: cProfile
```

**Threshold ConfigurÃ¡vel:**
```yaml
anomaly_detection:
  performance:
    latency:
      p95_threshold_ms: 1000
      spike_multiplier: 2.0    # Alerta se >2x baseline
      duration_minutes: 3
      severity: critical

      # Baseline learning (futuro)
      baseline:
        enabled: false
        learn_days: 7
```

---

### ğŸŸ¡ WARNINGS

#### D) Degrading Performance

**DescriÃ§Ã£o:** Performance piorando gradualmente

**DetecÃ§Ã£o:**
```yaml
condition:
  p95_latency: aumentou 20% em 10 minutos
  OR:
  error_rate: aumentou 50% em 10 minutos

exemplo:
  t=0:   p95=200ms, errors=1%
  t=10m: p95=250ms, errors=1.6%

  latency_increase: 25%      ğŸŸ¡
  error_increase: 60%        ğŸŸ¡

resultado: ğŸŸ¡ Performance degradando
```

**Causas:**
- TrÃ¡fego aumentando
- Memory leak gradual
- Cache warming

**AÃ§Ã£o Sugerida:**
```yaml
1. Monitorar tendÃªncia
2. Preparar para investigaÃ§Ã£o se continuar
3. Verificar se HPA estÃ¡ reagindo
```

---

## 4ï¸âƒ£ Anomalias de ConfiguraÃ§Ã£o (Config Issues)

### ğŸŸ¡ WARNINGS

#### A) Inefficient HPA Config

**DescriÃ§Ã£o:** ConfiguraÃ§Ã£o do HPA pode ser otimizada

**DetecÃ§Ã£o:**
```yaml
condition:
  minReplicas: == maxReplicas
  OR:
  target: < 30% OR > 90%
  OR:
  metrics: apenas CPU (sem memory)

exemplos:
  # HPA inÃºtil
  minReplicas: 5
  maxReplicas: 5              ğŸŸ¡ HPA nÃ£o faz nada!

  # Target muito baixo
  cpuTarget: 20%              ğŸŸ¡ DesperdÃ­cio

  # Target muito alto
  cpuTarget: 95%              ğŸŸ¡ Risco de sobrecarga

  # SÃ³ CPU (sem memory)
  metrics:
    - cpu: 70%
    # memory: ???            ğŸŸ¡ Deveria ter!
```

**AÃ§Ã£o Sugerida:**
```yaml
Min == Max:
  â†’ Remover HPA ou ajustar min/max

Target muito baixo:
  â†’ Aumentar para 70-80%

Target muito alto:
  â†’ Reduzir para 70-80%

Sem memory target:
  spec:
    metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
```

---

#### B) Resource Mismatch

**DescriÃ§Ã£o:** Requests/Limits desbalanceados

**DetecÃ§Ã£o:**
```yaml
condition:
  cpu_limit / cpu_request: > 4
  OR:
  memory_limit / memory_request: > 2
  OR:
  limits: nÃ£o definidos

exemplos:
  # CPU limit muito alto
  requests:
    cpu: 100m
  limits:
    cpu: 1000m              ğŸŸ¡ 10x! Pode causar throttling

  # Memory limit muito alto
  requests:
    memory: 128Mi
  limits:
    memory: 512Mi           ğŸŸ¡ 4x! Risco de OOM no node

  # Sem limits
  requests:
    cpu: 500m
  # limits: ???            ğŸŸ¡ Pode consumir tudo do node
```

**AÃ§Ã£o Sugerida:**
```yaml
RECOMENDAÃ‡ÃƒO:
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      # CPU: SEM LIMIT (evita throttling)
      memory: 1Gi    # 2x request (proteÃ§Ã£o OOM)
```

---

#### C) Missing HPA

**DescriÃ§Ã£o:** Deployment poderia se beneficiar de HPA

**DetecÃ§Ã£o:**
```yaml
condition:
  deployment_replicas: > 3
  has_hpa: false
  workload_type: Deployment (nÃ£o StatefulSet)

exemplo:
  apiVersion: apps/v1
  kind: Deployment
  spec:
    replicas: 5              ğŸŸ¡ Fixo, poderia ter HPA!

resultado: ğŸŸ¡ Considere adicionar HPA
```

**AÃ§Ã£o Sugerida:**
```yaml
Criar HPA:
  apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    name: my-app
  spec:
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: my-app
    minReplicas: 2
    maxReplicas: 10
    metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
```

---

## Limites e Thresholds Recomendados

### ConfiguraÃ§Ã£o Completa (`configs/watchdog.yaml`)

```yaml
anomaly_detection:
  # ==========================================
  # GENERAL SETTINGS
  # ==========================================
  enabled: true
  scan_interval_seconds: 30

  # Cooldown - Evitar spam de alertas
  cooldown:
    same_alert_minutes: 5        # NÃ£o alertar mesmo problema <5min
    same_hpa_minutes: 2          # Intervalo mÃ­nimo entre alertas do mesmo HPA
    max_alerts_per_hpa: 3        # MÃ¡ximo alertas ativos por HPA
    max_total_alerts: 100        # MÃ¡ximo total de alertas ativos

  # ==========================================
  # 1. SCALING ANOMALIES
  # ==========================================
  scaling:
    oscillation:
      enabled: true
      max_changes: 5
      window_minutes: 5
      severity: critical
      cooldown_minutes: 10

    maxed_out:
      enabled: true
      cpu_deviation_percent: 20
      memory_deviation_percent: 20
      duration_minutes: 2
      severity: critical
      cooldown_minutes: 5

    stuck:
      enabled: true
      cpu_deviation_percent: 30
      memory_deviation_percent: 30
      duration_minutes: 5
      severity: critical

    underutilization:
      enabled: true
      cpu_deviation_percent: 40
      memory_deviation_percent: 40
      duration_minutes: 15
      min_excess_replicas: 3
      severity: warning
      cooldown_minutes: 30

    frequent_scaling:
      enabled: true
      max_changes: 3
      window_minutes: 10
      severity: warning
      cooldown_minutes: 15

  # ==========================================
  # 2. POD HEALTH ANOMALIES
  # ==========================================
  pods:
    crash_loop:
      enabled: true
      max_restarts: 5
      window_minutes: 10
      severity: critical
      immediate_on_status: true  # Alerta imediato se status=CrashLoopBackOff

    oom_killed:
      enabled: true
      immediate: true
      memory_warning_percent: 95  # Alerta preventivo
      severity: critical

    not_ready:
      enabled: true
      ready_threshold_percent: 70
      duration_minutes: 3
      severity: critical

    restart_rate:
      enabled: true
      max_restarts: 2
      window_minutes: 15
      severity: warning
      cooldown_minutes: 10

    slow_rollout:
      enabled: true
      max_rollout_minutes: 10
      min_progress_percent: 50  # Pelo menos 50% em 10min
      severity: warning

  # ==========================================
  # 3. PERFORMANCE ANOMALIES
  # ==========================================
  performance:
    cpu_throttling:
      enabled: true
      threshold_percent: 25
      duration_minutes: 5
      severity: critical

    error_rate:
      enabled: true
      threshold_percent: 5
      duration_minutes: 2
      severity: critical
      cooldown_minutes: 5

    latency:
      enabled: true
      p95_threshold_ms: 1000
      p99_threshold_ms: 2000
      spike_multiplier: 2.0      # Alerta se >2x baseline
      duration_minutes: 3
      severity: critical

      # Baseline learning (futuro)
      baseline:
        enabled: false
        learn_days: 7
        percentile: 95

    degrading:
      enabled: true
      latency_increase_percent: 20
      error_increase_percent: 50
      window_minutes: 10
      severity: warning

  # ==========================================
  # 4. CONFIG ANOMALIES
  # ==========================================
  config:
    inefficient_hpa:
      enabled: true
      severity: warning
      checks:
        - min_equals_max
        - target_too_low: 30
        - target_too_high: 90
        - missing_memory_target

    resource_mismatch:
      enabled: true
      severity: warning
      cpu_limit_ratio: 4         # limit/request > 4 = warning
      memory_limit_ratio: 2      # limit/request > 2 = warning

    missing_hpa:
      enabled: true
      severity: info
      min_replicas_to_suggest: 3
      workload_types:
        - Deployment

  # ==========================================
  # ENRICHMENT
  # ==========================================
  enrichment:
    enabled: true
    include_metrics: true
    include_events: true
    include_logs_sample: false   # Futuro: sample de logs
    max_events: 10

  # ==========================================
  # CORRELATION
  # ==========================================
  correlation:
    enabled: true
    group_by:
      - cluster
      - namespace
      - hpa
    correlation_window_minutes: 5
    max_related_alerts: 5
```

---

## Roadmap de ImplementaÃ§Ã£o

### **Fase 1: MVP** âœ… (Implementar AGORA)

**Objetivo:** Detectar problemas crÃ­ticos mais comuns

**Anomalias:**
1. âœ… **Oscillation** - FÃ¡cil detectar, muito problemÃ¡tico
2. âœ… **Maxed Out** - CrÃ­tico, aÃ§Ã£o clara
3. âœ… **OOMKilled** - Urgente, fÃ¡cil detectar
4. âœ… **Pods Not Ready** - Muito comum, importante
5. âœ… **High Error Rate** - Indica problema real

**Arquivos:**
```
internal/analyzer/
â”œâ”€â”€ detector.go       # Engine principal de detecÃ§Ã£o
â”œâ”€â”€ rules/
â”‚   â”œâ”€â”€ oscillation.go
â”‚   â”œâ”€â”€ maxed_out.go
â”‚   â”œâ”€â”€ oom_killed.go
â”‚   â”œâ”€â”€ not_ready.go
â”‚   â””â”€â”€ error_rate.go
â”œâ”€â”€ models.go         # Anomaly, Alert structs
â””â”€â”€ detector_test.go  # Testes unitÃ¡rios
```

**Timeline:** 1-2 dias

---

### **Fase 2: ExpansÃ£o** (PrÃ³xima iteraÃ§Ã£o)

**Objetivo:** Cobrir mais cenÃ¡rios e performance

**Anomalias:**
6. Scaling Stuck
7. CPU Throttling
8. High Latency
9. Underutilization
10. CrashLoopBackOff

**Features:**
- Cooldown de alertas
- Alert deduplication
- HistÃ³rico de anomalias

**Timeline:** 2-3 dias

---

### **Fase 3: Features AvanÃ§adas** (Futuro)

**Objetivo:** InteligÃªncia e automaÃ§Ã£o

**Features:**

**1. Alert Correlation**
```yaml
# Agrupar alertas relacionados
Em vez de:
  ğŸš¨ HPA my-app: maxed out
  ğŸš¨ HPA my-app: high CPU
  ğŸš¨ HPA my-app: high latency

Criar incident agrupado:
  ğŸš¨ INCIDENT #123: my-app capacity issue
     Duration: 5 minutes
     Severity: CRITICAL

     Related alerts:
     â”œâ”€ HPA maxed out (10/10 replicas)
     â”œâ”€ CPU 92% (target 70%)
     â”œâ”€ P95 latency 1200ms (threshold 1000ms)
     â””â”€ Error rate 3.5%

     Root cause analysis:
     ğŸ’¡ HPA cannot scale further (maxReplicas reached)

     Suggested actions:
     1. Increase maxReplicas to 15
     2. Scale vertically (increase CPU per pod)
     3. Investigate if performance issue (not just load)

     Related metrics:
     [ASCII chart of CPU/Latency last 15min]
```

**2. Baseline Learning**
```yaml
# Aprender comportamento normal
baseline:
  enabled: true
  learn_for_days: 7

  patterns:
    - type: daily
      description: "Segunda = mais load que domingo"

    - type: hourly
      description: "9-17h = horÃ¡rio de pico"

    - type: weekly
      description: "Sexta tarde = deploy window"

  # Alertar apenas desvios significativos do normal
  deviation_threshold: 2.0  # 2 desvios padrÃ£o

  # Exemplo
  monday_9am:
    cpu_baseline: 75% Â± 5%
    current: 78%           âœ… Normal
    current: 92%           ğŸš¨ Anomalia (>2Ïƒ)
```

**3. Predictive Alerts**
```yaml
# Prever problemas antes de acontecer
prediction:
  enabled: true
  algorithms:
    - linear_regression
    - holt_winters

  predict_minutes: 15
  confidence_threshold: 0.8

  # Exemplo
  prediction:
    type: maxed_out
    confidence: 85%
    eta_minutes: 12

    alert:
      ğŸ”® PREDICTION: my-app will max out in 12 minutes
         Current: 7/10 replicas, CPU 82% (trending up)

         Proactive action:
         kubectl patch hpa my-app -p '{"spec":{"maxReplicas":15}}'
```

**4. Auto-Remediation** (Careful!)
```yaml
# AÃ§Ãµes automÃ¡ticas (com safeguards!)
auto_remediation:
  enabled: false  # Disabled por padrÃ£o
  dry_run: true   # Apenas log, nÃ£o executa

  max_actions_per_hour: 3
  require_confirmation: true

  rules:
    - anomaly: maxed_out
      action: increase_max_replicas
      params:
        increment: 5
        max_allowed: 50

    - anomaly: oom_killed
      action: increase_memory_limit
      params:
        multiplier: 1.5
        max_allowed: 4Gi
```

**5. Slack/Discord/Teams Integration**
```yaml
notifications:
  - type: slack
    webhook: https://hooks.slack.com/...
    channel: "#alerts-prod"
    severity: [critical, warning]

  - type: discord
    webhook: https://discord.com/api/webhooks/...

  - type: teams
    webhook: https://outlook.office.com/webhook/...

# Formato da mensagem
slack_message:
  ğŸš¨ *CRITICAL*: my-app maxed out

  *Details:*
  â€¢ Cluster: production
  â€¢ Namespace: default
  â€¢ HPA: my-app
  â€¢ Current: 10/10 replicas
  â€¢ CPU: 92% (target 70%)

  *Actions:*
  â€¢ <kubectl patch ...>
  â€¢ <view in grafana>
  â€¢ <silence for 1h>
```

**Timeline:** 1-2 semanas

---

## Ideias Futuras

### 1. Machine Learning Anomaly Detection

```yaml
ml_detection:
  enabled: false
  algorithm: isolation_forest

  features:
    - cpu_usage
    - memory_usage
    - request_rate
    - error_rate
    - latency_p95
    - replica_count

  # Treinar modelo com dados histÃ³ricos
  training:
    days: 30
    retrain_interval_days: 7

  # Detectar anomalias
  anomaly_threshold: 0.8  # Isolation score
```

### 2. Cost Optimization Alerts

```yaml
cost_optimization:
  enabled: true

  checks:
    - type: oversized_pods
      threshold: 50%  # CPU/Memory <50% por 24h

    - type: idle_hpas
      threshold: minReplicas por 7 dias

    - type: expensive_regions
      compare_regions: true

  # Exemplo alert
  ğŸ’° COST: my-app is oversized
     Average CPU: 25% (request: 1000m)

     Savings: $450/month by reducing to 500m

     Action:
     kubectl set resources deployment my-app --requests=cpu=500m
```

### 3. Chaos Engineering Integration

```yaml
chaos:
  enabled: false

  experiments:
    - name: pod_failure
      description: "Kill random pods to test resilience"
      validate_hpa_recovers: true

    - name: cpu_stress
      description: "Stress CPU to test HPA scaling"
      validate_scales_up: true

    - name: network_latency
      description: "Add latency to test degradation"
      validate_error_rate_acceptable: true
```

### 4. Multi-Cluster Comparison

```yaml
multi_cluster:
  enabled: true

  compare:
    - cluster: production-us
      with: production-eu

  alerts:
    - type: performance_drift
      threshold: 30%

  # Exemplo
  ğŸŒ DRIFT: my-app performance varies by cluster
     production-us: p95=200ms
     production-eu: p95=450ms  ğŸš¨ 2.25x slower!

     Investigation needed: network? config? data?
```

### 5. SLO/SLA Tracking

```yaml
slo:
  enabled: true

  objectives:
    - name: api_availability
      target: 99.9%
      window: 30d
      metric: (successful_requests / total_requests)

    - name: api_latency
      target: 95% < 500ms
      window: 7d
      metric: p95_latency

  # Burn rate alerts
  alerts:
    - type: error_budget_burn
      threshold: 10%  # Gastando 10% error budget/hora
```

---

## ReferÃªncias

### Artigos e Papers
- [Google SRE Book - Monitoring Distributed Systems](https://sre.google/sre-book/monitoring-distributed-systems/)
- [Robust and Effective Logging for Anomaly Detection](https://ieeexplore.ieee.org/document/8418609)
- [HPA Best Practices - Kubernetes Docs](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)

### Ferramentas Similares
- [Goldilocks](https://github.com/FairwindsOps/goldilocks) - VPA recommendations
- [Robusta](https://github.com/robusta-dev/robusta) - Kubernetes troubleshooting
- [Keptn](https://keptn.sh/) - Cloud-native application lifecycle

### Prometheus Queries
- [Awesome Prometheus Alerts](https://awesome-prometheus-alerts.grep.to/)
- [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics)

---

**Ãšltima atualizaÃ§Ã£o:** 2025-10-25
**VersÃ£o:** 1.0
**Status:** ğŸ“ DocumentaÃ§Ã£o Completa - Pronto para implementaÃ§Ã£o
