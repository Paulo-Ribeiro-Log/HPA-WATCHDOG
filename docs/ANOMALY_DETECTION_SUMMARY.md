# DetecÃ§Ã£o de Anomalias - SumÃ¡rio Executivo

ReferÃªncia rÃ¡pida das anomalias detectadas pelo HPA Watchdog.

## ğŸ¯ Quick Reference

### Fase 1 - MVP (Implementar AGORA)

| # | Anomalia | Severidade | DetecÃ§Ã£o | Limite |
|---|----------|------------|----------|--------|
| 1 | **Oscillation** | ğŸ”´ Critical | >5 mudanÃ§as rÃ©plicas | 5min |
| 2 | **Maxed Out** | ğŸ”´ Critical | replicas=max + CPU>target+20% | 2min |
| 3 | **OOMKilled** | ğŸ”´ Critical | Pod killed por OOM | Imediato |
| 4 | **Pods Not Ready** | ğŸ”´ Critical | <70% pods ready | 3min |
| 5 | **High Error Rate** | ğŸ”´ Critical | >5% erros 5xx | 2min |

### Fase 2 - ExpansÃ£o

| # | Anomalia | Severidade | DetecÃ§Ã£o | Limite |
|---|----------|------------|----------|--------|
| 6 | **Scaling Stuck** | ğŸ”´ Critical | CPU>target+30% sem escalar | 5min |
| 7 | **CPU Throttling** | ğŸ”´ Critical | >25% throttling | 5min |
| 8 | **High Latency** | ğŸ”´ Critical | P95>1000ms ou 2x baseline | 3min |
| 9 | **Underutilization** | ğŸŸ¡ Warning | CPU<target-40% + >3 replicas | 15min |
| 10 | **CrashLoopBackOff** | ğŸ”´ Critical | Pod em crash loop | Imediato |

## ğŸ“Š Decision Matrix

```
Problema Detectado â†’ AÃ§Ã£o Sugerida

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCALING                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Oscillation        â†’ Aumentar stabilizationWindow           â”‚
â”‚ Maxed Out          â†’ Aumentar maxReplicas                   â”‚
â”‚ Scaling Stuck      â†’ Verificar quota/capacidade cluster     â”‚
â”‚ Underutilization   â†’ Reduzir maxReplicas ou target          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POD HEALTH                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OOMKilled          â†’ Aumentar memory limit                  â”‚
â”‚ CrashLoopBackOff   â†’ Verificar logs + dependÃªncias          â”‚
â”‚ Pods Not Ready     â†’ Ajustar readiness probe                â”‚
â”‚ High Restart Rate  â†’ Investigar causa (OOM? probe?)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PERFORMANCE                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CPU Throttling     â†’ Remover CPU limit ou aumentar          â”‚
â”‚ High Error Rate    â†’ Scale up ou verificar dependÃªncias     â”‚
â”‚ High Latency       â†’ Scale up ou profile aplicaÃ§Ã£o          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš¨ Alert Priority

```
P0 (Imediato):
  â”œâ”€ OOMKilled
  â”œâ”€ CrashLoopBackOff
  â””â”€ High Error Rate (>10%)

P1 (Urgente - <5min):
  â”œâ”€ Maxed Out
  â”œâ”€ Pods Not Ready
  â”œâ”€ High Error Rate (>5%)
  â””â”€ Scaling Stuck

P2 (Importante - <15min):
  â”œâ”€ Oscillation
  â”œâ”€ CPU Throttling
  â””â”€ High Latency

P3 (AtenÃ§Ã£o - <1h):
  â”œâ”€ Underutilization
  â”œâ”€ Frequent Scaling
  â””â”€ Config Issues
```

## ğŸ’¡ Common Correlations

```
1. HPA maxed + CPU high + Latency high
   â†’ Capacity issue, need to scale

2. HPA scaled up + Pods not ready
   â†’ Readiness probe or dependencies issue

3. HPA scaled up + Pods OOMKilled
   â†’ Memory limit too low or leak

4. CPU usage low + CPU throttling high
   â†’ CPU limit too low (remove limit!)

5. Oscillation + Frequent scaling
   â†’ HPA config too sensitive
```

## ğŸ“ˆ Baseline Values

### Thresholds Recomendados

```yaml
# Scaling
oscillation_max_changes: 5 (5min)
maxed_out_deviation: 20% (2min)
stuck_deviation: 30% (5min)
underutilization_deviation: 40% (15min)

# Pod Health
crash_loop_max_restarts: 5 (10min)
not_ready_threshold: 70% (3min)
restart_rate: 2 (15min)

# Performance
cpu_throttling: 25% (5min)
error_rate: 5% (2min)
latency_p95: 1000ms (3min)
```

### HPA Config Ideal

```yaml
# Good defaults
minReplicas: 2
maxReplicas: 10
cpuTarget: 70%
memoryTarget: 80%

stabilizationWindow:
  scaleDown: 300s  # 5min
  scaleUp: 0s      # Imediato

# Resources (per pod)
requests:
  cpu: 500m
  memory: 512Mi
limits:
  # cpu: NONE (evita throttling)
  memory: 1Gi  # 2x request
```

## ğŸ”„ Alert Lifecycle

```
1. DETECT (scan_interval: 30s)
   â”œâ”€ Scan HPAs + Deployments
   â”œâ”€ Apply detection rules
   â””â”€ Check thresholds + duration

2. VALIDATE (time-based)
   â”œâ”€ Confirm problem persists
   â”œâ”€ Duration > threshold?
   â””â”€ Not in cooldown period?

3. ENRICH
   â”œâ”€ Add metrics context
   â”œâ”€ Add K8s events
   â”œâ”€ Correlate related alerts
   â””â”€ Suggest actions

4. ALERT
   â”œâ”€ Create UnifiedAlert
   â”œâ”€ Set severity
   â”œâ”€ Add to dashboard
   â””â”€ (Future) Send notification

5. COOLDOWN
   â”œâ”€ Start cooldown timer
   â”œâ”€ Prevent duplicate alerts
   â””â”€ Auto-ack if resolved
```

## ğŸ“ File Structure

```
internal/analyzer/
â”œâ”€â”€ detector.go              # Engine principal
â”œâ”€â”€ detector_test.go         # Testes
â”œâ”€â”€ models.go                # Anomaly, Alert structs
â”œâ”€â”€ config.go                # Thresholds config
â”œâ”€â”€ rules/                   # Regras de detecÃ§Ã£o
â”‚   â”œâ”€â”€ scaling/
â”‚   â”‚   â”œâ”€â”€ oscillation.go
â”‚   â”‚   â”œâ”€â”€ maxed_out.go
â”‚   â”‚   â”œâ”€â”€ stuck.go
â”‚   â”‚   â””â”€â”€ underutilization.go
â”‚   â”œâ”€â”€ pods/
â”‚   â”‚   â”œâ”€â”€ oom_killed.go
â”‚   â”‚   â”œâ”€â”€ crash_loop.go
â”‚   â”‚   â”œâ”€â”€ not_ready.go
â”‚   â”‚   â””â”€â”€ restart_rate.go
â”‚   â””â”€â”€ performance/
â”‚       â”œâ”€â”€ cpu_throttling.go
â”‚       â”œâ”€â”€ error_rate.go
â”‚       â””â”€â”€ latency.go
â”œâ”€â”€ correlation.go           # Alert correlation
â””â”€â”€ enrichment.go            # Context enrichment
```

## ğŸ§ª Testing Strategy

```bash
# Unit tests (cada regra)
go test ./internal/analyzer/rules/... -v

# Integration tests (detector completo)
go test ./internal/analyzer/... -v

# Real cluster test
./build/hpa-watchdog test \
  --cluster production \
  --namespace default \
  --prometheus

# Simulate anomaly
kubectl apply -f tests/fixtures/oscillating-hpa.yaml
# Wait 5min
# Verify alert detected
```

## ğŸ“š Links Ãšteis

- [DocumentaÃ§Ã£o Completa](./ANOMALY_DETECTION.md)
- [Guia de Testes](./TESTING.md)
- [ConfiguraÃ§Ã£o](../configs/watchdog.yaml)
- [CLAUDE.md](../CLAUDE.md)

---

**Quick Start:**

1. Ler [ANOMALY_DETECTION.md](./ANOMALY_DETECTION.md) completo
2. Implementar Fase 1 (5 anomalias MVP)
3. Testar em cluster staging
4. Deploy em produÃ§Ã£o
5. Iterar com Fase 2

**DÃºvidas?** Consulte a documentaÃ§Ã£o completa ou cÃ³digo implementado.
