# EstratÃ©gia de Coleta e Armazenamento de Dados

Arquitetura para coleta, armazenamento e anÃ¡lise de dados do HPA Watchdog.

## ğŸ¯ Objetivos

1. **Performance** - Scan rÃ¡pido (< 5s por cluster)
2. **HistÃ³rico** - Dados suficientes para detectar anomalias
3. **Baixo overhead** - NÃ£o sobrecarregar K8s API / Prometheus
4. **AnÃ¡lise temporal** - Comparar com baseline
5. **Persistence opcional** - In-memory primeiro, DB depois

---

## ğŸ“Š Modelo de Dados

### HPASnapshot (por scan)

```go
type HPASnapshot struct {
    // Metadata
    Timestamp   time.Time
    Cluster     string
    Namespace   string
    Name        string

    // K8s Data
    MinReplicas     int32
    MaxReplicas     int32
    CurrentReplicas int32
    DesiredReplicas int32
    CPUTarget       int32
    MemoryTarget    int32

    // Prometheus - Current
    CPUCurrent    float64
    MemoryCurrent float64

    // Prometheus - History (5min, 10 pontos)
    CPUHistory     []float64  // [t-300s, t-270s, ..., t-0s]
    MemoryHistory  []float64
    ReplicaHistory []int32

    // Extended Metrics
    RequestRate float64
    ErrorRate   float64
    P95Latency  float64

    // Resources
    CPURequest    string
    CPULimit      string
    MemoryRequest string
    MemoryLimit   string

    // Status
    Ready         bool
    ScalingActive bool
    LastScaleTime *time.Time

    DataSource DataSource // Prometheus, MetricsServer, Hybrid
}
```

### TimeSeriesData (in-memory cache)

```go
type TimeSeriesData struct {
    HPAKey      string // "cluster/namespace/name"
    Snapshots   []HPASnapshot
    MaxDuration time.Duration // 5 minutos

    // EstatÃ­sticas calculadas
    Stats HPAStats

    sync.RWMutex
}

type HPAStats struct {
    // Calculado dos Ãºltimos 5 min
    CPUAverage    float64
    CPUMin        float64
    CPUMax        float64
    CPUStdDev     float64

    ReplicaChanges int      // Quantas mudanÃ§as em 5min
    LastChange     time.Time

    // Trend
    CPUTrend      string // "increasing", "decreasing", "stable"
    ReplicaTrend  string
}
```

---

## ğŸ”„ Fluxo de Coleta de Dados

### OpÃ§Ã£o 1: **In-Memory Only** (MVP - Recomendado) â­

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SCAN LOOP (30s)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. Para cada cluster:                                      â”‚
â”‚     â”œâ”€ Lista HPAs                                           â”‚
â”‚     â”œâ”€ Para cada HPA:                                       â”‚
â”‚     â”‚  â”œâ”€ Coleta K8s data                                   â”‚
â”‚     â”‚  â”œâ”€ Coleta Prometheus metrics (se disponÃ­vel)         â”‚
â”‚     â”‚  â”œâ”€ Cria HPASnapshot                                  â”‚
â”‚     â”‚  â””â”€ Armazena in-memory                                â”‚
â”‚     â”‚                                                        â”‚
â”‚  2. In-Memory Storage (5 minutos):                          â”‚
â”‚     â”œâ”€ TimeSeriesData map[string]*TimeSeriesData           â”‚
â”‚     â”œâ”€ Key: "cluster/namespace/hpa"                         â”‚
â”‚     â”œâ”€ Value: Ãºltimos 10 snapshots (5min @ 30s interval)   â”‚
â”‚     â””â”€ Auto-cleanup: remove snapshots > 5min                â”‚
â”‚                                                              â”‚
â”‚  3. AnÃ¡lise (apÃ³s cada scan):                               â”‚
â”‚     â”œâ”€ Calcula stats (avg, min, max, stddev)               â”‚
â”‚     â”œâ”€ Detecta anomalias comparando com histÃ³rico          â”‚
â”‚     â”œâ”€ Cria alerts se necessÃ¡rio                            â”‚
â”‚     â””â”€ Envia para TUI                                       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   MEMORY STRUCTURE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  timeSeriesCache = {                                        â”‚
â”‚      "prod/default/api": {                                  â”‚
â”‚          Snapshots: [                                       â”‚
â”‚              {t: 10:00:00, cpu: 70%, replicas: 5},         â”‚
â”‚              {t: 10:00:30, cpu: 72%, replicas: 5},         â”‚
â”‚              {t: 10:01:00, cpu: 75%, replicas: 6}, â† scale â”‚
â”‚              ...                                            â”‚
â”‚              {t: 10:05:00, cpu: 68%, replicas: 6},         â”‚
â”‚          ],                                                 â”‚
â”‚          Stats: {                                           â”‚
â”‚              CPUAverage: 71.2%,                            â”‚
â”‚              ReplicaChanges: 1,                            â”‚
â”‚              CPUTrend: "stable"                            â”‚
â”‚          }                                                  â”‚
â”‚      },                                                     â”‚
â”‚      "prod/default/worker": {...},                         â”‚
â”‚      ...                                                    â”‚
â”‚  }                                                          â”‚
â”‚                                                              â”‚
â”‚  Memory Usage:                                              â”‚
â”‚    - 250 HPAs Ã— 10 snapshots Ã— ~500 bytes = ~1.25 MB      â”‚
â”‚    - Totalmente aceitÃ¡vel!                                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**PrÃ³s:**
- âœ… **Simples** - Sem DB, sem dependencies
- âœ… **RÃ¡pido** - Acesso instantÃ¢neo
- âœ… **Baixo overhead** - SÃ³ memÃ³ria
- âœ… **Suficiente para MVP** - 5min histÃ³rico detecta maioria das anomalias

**Contras:**
- âŒ **Perde dados ao reiniciar** - HistÃ³rico perdido
- âŒ **Sem long-term trends** - NÃ£o detecta tendÃªncias de dias/semanas
- âŒ **NÃ£o persiste alertas** - Alertas somem ao reiniciar

**Quando usar:**
- âœ… **MVP / Fase 1**
- âœ… Detectar anomalias imediatas (oscillation, maxed out, etc)
- âœ… Monitoramento real-time

---

### OpÃ§Ã£o 2: **Hybrid (In-Memory + SQLite)** (Fase 2) ğŸ’¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  HYBRID STORAGE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  IN-MEMORY (hot data - 5 minutos):                         â”‚
â”‚  â”œâ”€ TimeSeriesData cache                                    â”‚
â”‚  â”œâ”€ AnÃ¡lise em tempo real                                   â”‚
â”‚  â””â”€ DetecÃ§Ã£o de anomalias imediatas                        â”‚
â”‚      â”‚                                                       â”‚
â”‚      â”‚ (cada 5 minutos)                                     â”‚
â”‚      â–¼                                                       â”‚
â”‚  SQLITE (warm data - 7 dias):                              â”‚
â”‚  â”œâ”€ snapshots table                                         â”‚
â”‚  â”œâ”€ anomalies table                                         â”‚
â”‚  â”œâ”€ baselines table (aprendizado)                          â”‚
â”‚  â””â”€ alerts table (histÃ³rico)                               â”‚
â”‚      â”‚                                                       â”‚
â”‚      â”‚ (apÃ³s 7 dias)                                        â”‚
â”‚      â–¼                                                       â”‚
â”‚  ARCHIVE (cold data - opcional):                           â”‚
â”‚  â””â”€ Compressed JSON/Parquet files                          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Schema SQLite:**

```sql
-- Snapshots agregados (nÃ£o todos os pontos)
CREATE TABLE snapshots (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp DATETIME NOT NULL,
    cluster TEXT NOT NULL,
    namespace TEXT NOT NULL,
    hpa_name TEXT NOT NULL,

    -- Aggregated metrics (5min window)
    cpu_avg REAL,
    cpu_min REAL,
    cpu_max REAL,
    cpu_p95 REAL,

    memory_avg REAL,
    memory_min REAL,
    memory_max REAL,

    replicas_current INTEGER,
    replicas_desired INTEGER,
    replica_changes INTEGER, -- mudanÃ§as nos Ãºltimos 5min

    request_rate REAL,
    error_rate REAL,
    p95_latency REAL,

    INDEX idx_hpa (cluster, namespace, hpa_name, timestamp)
);

-- Anomalias detectadas
CREATE TABLE anomalies (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    detected_at DATETIME NOT NULL,
    resolved_at DATETIME,

    cluster TEXT NOT NULL,
    namespace TEXT NOT NULL,
    hpa_name TEXT NOT NULL,

    type TEXT NOT NULL, -- "oscillation", "maxed_out", etc
    severity TEXT NOT NULL, -- "critical", "warning", "info"

    description TEXT,
    suggested_action TEXT,

    -- Snapshot no momento da detecÃ§Ã£o
    snapshot_id INTEGER,

    -- Estado
    acknowledged BOOLEAN DEFAULT FALSE,
    acknowledged_at DATETIME,
    acknowledged_by TEXT,

    INDEX idx_anomaly (cluster, namespace, hpa_name, detected_at)
);

-- Baselines (aprendizado de padrÃ£o normal)
CREATE TABLE baselines (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    cluster TEXT NOT NULL,
    namespace TEXT NOT NULL,
    hpa_name TEXT NOT NULL,

    -- PadrÃ£o temporal
    pattern_type TEXT, -- "hourly", "daily", "weekly"
    pattern_key TEXT,  -- "monday_09h", "weekday_peak", etc

    -- EstatÃ­sticas do padrÃ£o
    cpu_baseline REAL,
    cpu_stddev REAL,
    replicas_baseline INTEGER,

    samples INTEGER, -- quantas amostras formaram o baseline
    last_updated DATETIME,

    UNIQUE(cluster, namespace, hpa_name, pattern_type, pattern_key)
);

-- HistÃ³rico de alertas (para deduplicaÃ§Ã£o)
CREATE TABLE alert_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    created_at DATETIME NOT NULL,

    cluster TEXT NOT NULL,
    namespace TEXT NOT NULL,
    hpa_name TEXT NOT NULL,
    type TEXT NOT NULL,

    fingerprint TEXT NOT NULL, -- hash do alerta

    INDEX idx_fingerprint (fingerprint, created_at)
);
```

**Workflow:**

```go
// Scan loop
for {
    // 1. Coleta in-memory (como antes)
    snapshot := collectSnapshot(hpa)
    tsData.Add(snapshot)

    // 2. Detecta anomalias
    anomalies := analyzer.Detect(tsData)

    // 3. Persiste no SQLite (async, nÃ£o bloqueia)
    go func() {
        // Salva snapshot agregado (cada 5min)
        if time.Since(lastPersist) > 5*time.Minute {
            db.SaveAggregatedSnapshot(tsData.Stats)
        }

        // Salva anomalias
        for _, anomaly := range anomalies {
            db.SaveAnomaly(anomaly)
        }
    }()

    // 4. Atualiza baseline (aprende padrÃ£o)
    if shouldUpdateBaseline() {
        baseline.Learn(tsData)
    }

    time.Sleep(30 * time.Second)
}
```

**PrÃ³s:**
- âœ… **PersistÃªncia** - Dados sobrevivem restart
- âœ… **Long-term trends** - Detecta padrÃµes de dias/semanas
- âœ… **Baseline learning** - Aprende comportamento normal
- âœ… **Alert history** - HistÃ³rico completo
- âœ… **Leve** - SQLite Ã© single-file, sem servidor

**Contras:**
- âš ï¸ **Mais complexo** - Gerenciar DB, migrations
- âš ï¸ **Disk I/O** - Pode ser gargalo (mitiga com async writes)
- âš ï¸ **Single-node** - SQLite nÃ£o Ã© distribuÃ­do

**Quando usar:**
- âœ… **Fase 2** - ApÃ³s MVP validado
- âœ… Baseline learning
- âœ… HistÃ³rico de alertas
- âœ… Trends de longo prazo

---

### OpÃ§Ã£o 3: **External TSDB (Prometheus)** (AvanÃ§ado) ğŸ“ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LEVERAGE PROMETHEUS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Watchdog NÃƒO armazena mÃ©tricas!                           â”‚
â”‚  â”œâ”€ Usa Prometheus como source of truth                    â”‚
â”‚  â”œâ”€ Queries PromQL para anÃ¡lise                            â”‚
â”‚  â””â”€ Recording rules para agregaÃ§Ãµes                         â”‚
â”‚                                                              â”‚
â”‚  Armazena APENAS:                                           â”‚
â”‚  â”œâ”€ Estado de alertas (in-memory)                          â”‚
â”‚  â”œâ”€ Baselines aprendidos (SQLite leve)                     â”‚
â”‚  â””â”€ ConfiguraÃ§Ã£o de thresholds                             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Recording Rules no Prometheus:**

```yaml
# prometheus-rules.yaml
groups:
  - name: hpa_watchdog
    interval: 30s
    rules:
      # Replica changes in 5min
      - record: hpa_watchdog:replica_changes:5m
        expr: |
          changes(kube_horizontalpodautoscaler_status_current_replicas[5m])

      # CPU deviation from target
      - record: hpa_watchdog:cpu_deviation:percent
        expr: |
          (
            sum by (namespace, horizontalpodautoscaler) (
              rate(container_cpu_usage_seconds_total[1m])
            ) /
            sum by (namespace, horizontalpodautoscaler) (
              kube_pod_container_resource_requests{resource="cpu"}
            ) * 100
          ) -
          on(namespace, horizontalpodautoscaler)
          kube_horizontalpodautoscaler_spec_target_metric{metric_name="cpu"}

      # Maxed out detection
      - record: hpa_watchdog:maxed_out:bool
        expr: |
          (
            kube_horizontalpodautoscaler_status_current_replicas
            ==
            kube_horizontalpodautoscaler_spec_max_replicas
          )
          and
          (hpa_watchdog:cpu_deviation:percent > 20)
```

**Watchdog faz:**

```go
// Apenas queries, sem storage de mÃ©tricas
func (d *Detector) DetectMaxedOut(cluster, namespace, hpa string) (*Anomaly, error) {
    query := `hpa_watchdog:maxed_out:bool{cluster="` + cluster + `",namespace="` + namespace + `",hpa="` + hpa + `"}`

    result, err := prometheus.Query(query)
    if err != nil {
        return nil, err
    }

    if result > 0 {
        // Cria anomaly
        return &Anomaly{
            Type: AnomalyMaxedOut,
            // ...
        }, nil
    }

    return nil, nil
}
```

**PrÃ³s:**
- âœ… **Zero storage** - Prometheus jÃ¡ tem tudo
- âœ… **EscalÃ¡vel** - Prometheus Ã© TSDB otimizado
- âœ… **Rich queries** - PromQL poderoso
- âœ… **JÃ¡ existe** - Aproveita infra existente

**Contras:**
- âš ï¸ **DependÃªncia** - Precisa de Prometheus
- âš ï¸ **LatÃªncia** - Queries podem ser lentas
- âš ï¸ **Complexidade** - Recording rules, federation

**Quando usar:**
- âœ… **Fase 3** - ProduÃ§Ã£o em escala
- âœ… MÃºltiplos clusters
- âœ… JÃ¡ tem Prometheus bem configurado

---

## ğŸ¯ DetecÃ§Ã£o de Anomalias com Dados Coletados

### Exemplo: Oscillation Detection

**In-Memory (OpÃ§Ã£o 1):**

```go
func (d *Detector) DetectOscillation(tsData *TimeSeriesData) *Anomaly {
    // Ãšltimos 10 snapshots (5 minutos)
    snapshots := tsData.GetHistory()

    // Conta mudanÃ§as de rÃ©plicas
    changes := 0
    for i := 1; i < len(snapshots); i++ {
        if snapshots[i].CurrentReplicas != snapshots[i-1].CurrentReplicas {
            changes++
        }
    }

    // Threshold: >5 mudanÃ§as em 5min
    if changes > 5 {
        return &Anomaly{
            Type:     AnomalyOscillation,
            Severity: SeverityCritical,
            Description: fmt.Sprintf(
                "HPA oscillating: %d replica changes in 5 minutes",
                changes,
            ),
            // EvidÃªncia
            Evidence: map[string]interface{}{
                "changes": changes,
                "history": extractReplicaHistory(snapshots),
            },
        }
    }

    return nil
}

// Helper
func extractReplicaHistory(snapshots []HPASnapshot) []int32 {
    history := make([]int32, len(snapshots))
    for i, s := range snapshots {
        history[i] = s.CurrentReplicas
    }
    return history
    // Ex: [3, 5, 3, 6, 4, 7, 3, 5, 4, 6] â†’ 9 mudanÃ§as!
}
```

**Com SQLite (OpÃ§Ã£o 2) - Baseline Learning:**

```go
func (d *Detector) DetectMaxedOutWithBaseline(ctx context.Context, tsData *TimeSeriesData) *Anomaly {
    current := tsData.GetLatest()

    // Busca baseline do padrÃ£o atual
    baseline, err := d.db.GetBaseline(ctx,
        current.Cluster,
        current.Namespace,
        current.Name,
        getCurrentPattern(), // "monday_09h"
    )

    if err != nil || baseline == nil {
        // Sem baseline ainda, usa threshold fixo
        return d.detectMaxedOutSimple(current)
    }

    // Compara com baseline aprendido
    deviation := (current.CPUCurrent - baseline.CPUBaseline) / baseline.CPUStdDev

    // Maxed out + desvio significativo do normal
    if current.CurrentReplicas == current.MaxReplicas && deviation > 2.0 {
        return &Anomaly{
            Type:     AnomalyMaxedOut,
            Severity: SeverityCritical,
            Description: fmt.Sprintf(
                "HPA maxed out with CPU %.1f%% (%.1fÏƒ above baseline %.1f%%)",
                current.CPUCurrent,
                deviation,
                baseline.CPUBaseline,
            ),
            Evidence: map[string]interface{}{
                "current_cpu": current.CPUCurrent,
                "baseline_cpu": baseline.CPUBaseline,
                "stddev": baseline.CPUStdDev,
                "deviation_sigma": deviation,
            },
        }
    }

    return nil
}

func getCurrentPattern() string {
    now := time.Now()

    // PadrÃµes:
    // - "weekday_09h", "weekday_14h", etc (hora de pico)
    // - "weekend_low" (final de semana)
    // - "monday_peak" (segunda-feira)

    isWeekend := now.Weekday() == time.Saturday || now.Weekday() == time.Sunday
    if isWeekend {
        return "weekend_low"
    }

    hour := now.Hour()
    if hour >= 9 && hour <= 17 {
        return fmt.Sprintf("weekday_%02dh", hour)
    }

    return "weekday_off_hours"
}
```

---

## ğŸ“‹ RecomendaÃ§Ã£o de ImplementaÃ§Ã£o

### **Fase 1 - MVP** (IMPLEMENTAR AGORA)

**Storage:** In-Memory apenas

```go
// storage/memory.go
type MemoryStorage struct {
    timeSeries map[string]*TimeSeriesData
    mu         sync.RWMutex
}

func (s *MemoryStorage) Add(snapshot *HPASnapshot) {
    key := fmt.Sprintf("%s/%s/%s",
        snapshot.Cluster,
        snapshot.Namespace,
        snapshot.Name,
    )

    s.mu.Lock()
    defer s.mu.Unlock()

    if _, exists := s.timeSeries[key]; !exists {
        s.timeSeries[key] = NewTimeSeriesData(key, 5*time.Minute)
    }

    s.timeSeries[key].Add(*snapshot)
}

func (s *MemoryStorage) Get(cluster, namespace, name string) *TimeSeriesData {
    key := fmt.Sprintf("%s/%s/%s", cluster, namespace, name)

    s.mu.RLock()
    defer s.mu.RUnlock()

    return s.timeSeries[key]
}
```

**Detector:**

```go
// analyzer/detector.go
type Detector struct {
    storage Storage
    config  *AnomalyConfig
}

func (d *Detector) Scan() []*Anomaly {
    var anomalies []*Anomaly

    // Para cada HPA monitorado
    for key, tsData := range d.storage.GetAll() {
        // Aplica cada regra
        if a := d.detectOscillation(tsData); a != nil {
            anomalies = append(anomalies, a)
        }

        if a := d.detectMaxedOut(tsData); a != nil {
            anomalies = append(anomalies, a)
        }

        // ... mais regras
    }

    return anomalies
}
```

**Vantagens:**
- âœ… Simples de implementar (1-2 dias)
- âœ… Sem dependÃªncias externas
- âœ… Performance excelente
- âœ… Suficiente para 90% dos casos

---

### **Fase 2 - Production** (Depois do MVP)

**Storage:** Hybrid (In-Memory + SQLite)

**Adicionar:**
- Persistence de snapshots agregados
- Baseline learning (7 dias de aprendizado)
- Alert history com deduplicaÃ§Ã£o
- Recovery de estado apÃ³s restart

**Timeline:** +3-5 dias apÃ³s MVP

---

### **Fase 3 - Scale** (Futuro)

**Storage:** Leverage Prometheus + SQLite leve

**Adicionar:**
- Recording rules no Prometheus
- Queries otimizadas
- Multi-cluster federation
- Long-term storage (S3/GCS)

**Timeline:** 1-2 semanas

---

## ğŸ§ª ComparaÃ§Ã£o de Performance

| MÃ©trica | In-Memory | SQLite | Prometheus |
|---------|-----------|--------|------------|
| **Write latency** | < 1ms | 5-10ms | 50-100ms |
| **Read latency** | < 1ms | 2-5ms | 100-500ms |
| **Memory (250 HPAs)** | ~1-2 MB | ~5-10 MB | N/A |
| **Disk usage** | 0 | ~100 MB/day | N/A |
| **Query power** | Simple | SQL | PromQL |
| **Restart recovery** | âŒ Lost | âœ… Full | âœ… Full |
| **Long-term trends** | âŒ 5min | âœ… Days | âœ… Weeks |

---

## ğŸ’¡ ConclusÃ£o

### Para MVP (AGORA):
**ğŸ¯ OpÃ§Ã£o 1: In-Memory Only**

- Simples, rÃ¡pido, sem dependencies
- 5 minutos de histÃ³rico = suficiente
- Detecta 90% das anomalias importantes
- Implementa em 1-2 dias

### Para ProduÃ§Ã£o (DEPOIS):
**ğŸ¯ OpÃ§Ã£o 2: Hybrid (In-Memory + SQLite)**

- Persistence + long-term trends
- Baseline learning
- Alert history
- Implementa em +3-5 dias

### Para Escala (FUTURO):
**ğŸ¯ OpÃ§Ã£o 3: Prometheus + SQLite leve**

- Aproveita infra existente
- EscalÃ¡vel para muitos clusters
- Recording rules otimizadas

---

**RecomendaÃ§Ã£o:** ComeÃ§ar com In-Memory, migrar para Hybrid quando necessÃ¡rio! ğŸš€

